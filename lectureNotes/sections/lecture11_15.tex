\documentclass[../main.tex]{subfiles}

\begin{document}

\section{Lecture 11}{Eigenvalue Problems}


\begin{definition}
    ALgebraic multiplicity of an eigenvalue counts the number of times the eigenvalue occurs as a root of the characteristic equation. Geometric multiplicity counts how many linearly independent eigenvectors correspond to an eigenvalue. It is a proveable fact that 
    algebraic multiplicity is at least as much as the geometric multiplicity. If algebraic multiplicity is greater than geometric multiplicity, we say that the matrix is defective.
\end{definition}

\begin{theorem}
    Similar matrices share the same eigenvalues.
\end{theorem}

\begin{theorem}
    If a matrix is defective, then it cannot have an eigenvector basis.
\end{theorem}

\begin{proposition}
    A matrix is diagonalizable iff it has an eigenvector basis.
\end{proposition}

\begin{proposition}
    The following matrix transformations change eigenvalues and eigenvectors as follows:

    \begin{outline}
        \1 $A \rightarrow (A - \sigma I)$ causes $\lambda \rightarrow \lambda - \sigma$. 
        \1 $A \rightarrow \inv{A}$ causes $\lambda \to \frac{1}{\lambda}$>
        \1 $A \rightarrow A^k$ causes $\lambda \to \lambda^k$.
        \1 If $A = PX\inv{P}$, then $X$ has the same eigenvalues but every eigenvector $v$ now becomes $\inv{P}v$.
    \end{outline}
\end{proposition}

\begin{proposition}
    Suppose that we perturb a diagonal matrix $A$ with some matrix $E$. Then the distance between any eigenvalue $u$ of $A + E$ to an eigenvalue of $A$ $\lambda_k$ closest to $u$ is bounded by $k(A) \norm{E}$.
\end{proposition}

\section{Lecture 12}

\begin{problem}
    Characteristic Polynomial
For which of the following reasons is the characteristic polynomial of a matrix NOT useful, in general, for computing the eigenvalues of the matrix?

Select all that apply:
Its coefficients may not be well determined numerically.
Its roots may be difficult to compute.
Its roots may be sensitive to perturbations in the coefficients.
None of these
\end{problem}

\begin{solution}
    Yes, the roots change if the coefficients change -- hence any perturbation will affect the roots; determining the coefficients numerically is also problematic, simply because numerical comptutation requires rounding error and truncation error. The second is a given.
\end{solution}

\begin{problem}
    Problem Transformations and Spectral Radius
Which of the following transformations preserve the spectral radius of a matrix A?

Select all that apply:
Powers
None of these
Shift
Polynomial
Inversion
\end{problem}
\begin{solution}
    Obviously, none of them, since they all change the eigenvalues.
\end{solution}

\begin{problem}
    Diagonalizability
Of the classes of n×n matrices listed below, which is the smallest class of matrices that are not necessarily diagonalizable by a similarity transformation?

Choice*
normal matrices
all matrices
real symmetric matrices
matrices with n distinct eigenvalues
\end{problem}
\begin{solution}
    A spectral theorem asserts that normal matrices are unitarily diagonalizable; a theorem asserts that real symmetric matrices have an orthogonal basis -- hence they are diagonalizable; in general, any matrix with an eigenbasis is diagonalizable.
\end{solution}

\begin{problem}
    Let $A = XD\inv{X}$. Suppose $\hat{A} = A + \delta A = \hat{X} \hat{D} \inv{(\hat{X})}$. Which matrix is $\hat{A}$ similar to?
\end{problem}

\begin{solution}
    $\inv{X}\hat{A} X = \inv{X}AX + \inv{X}\delta A X = D + \inv{X} \delta A X$
\end{solution}

\begin{definition}
    The eigenvector corresponding to the largest eigenvalue will hence forth be called the maximal eigenvector. 
\end{definition}

\begin{remark}
    Power iteration can fail because of certain reasons; certain of these can be remedied:

    \begin{outline}
        \1 No component alongside the dominant eigenvector.
        \2 Rounding error usually introduces some component -- and a random vector will usually include the component.
        \1 Overflow of entries in the vector being iterated upon:
        \2 Normalize the vector
        \1 There is no one dominant eigenvector, because two distinct eigenvalues of equal, maximal magnitude exist.
    \end{outline}
    
\end{remark}

\begin{definition}
    The rayleight quotient is the quantity

    \[
        \frac{x^T Ax}{x^Tx}
    \]
\end{definition}

\begin{remark}
    Let $e_k = \norm{v_1^{k} - x_1}$ where $v_1^k$ is the estimate of $x_1$ at the $k$th iteration.

    It can be shown that error $\approx c \abs{\frac{\lambda_2}{\lambda_1}}^k$, which implies that

    \[
        \frac{\norm{e_{k+1}}}{\norm{e_k}} = \abs{\frac{\lambda_2}{\lambda_1}}
    \]

    This is called linear convergence, because the error in the next iteration is a linear (think $y = ax$) scaling of the previous error.

    Power iteration obviously costs $O(n^2)$ at each iteration, because we're repeatedly multiplying a matrix by a vector.
\end{remark}

\begin{definition}
    Inverse power iteration is the algorithm that repeatedly hits $x$ with $\inv{A}$. Its statistics:

    \begin{outline}
        \1 The error rate is 

        \[
            \frac{\abs{1/\lambda_{n-1}}}{\abs{1/\lambda_{n}}} = \frac{\abs{\lambda_n}}{\abs{\lambda_{n-1}}} 
        \]
        \1 The cost is initially $O(n^3)$ since we solve for $y$ in $Ay = x$. Thereafter, the cost is $O(n^2)$.
        \1 The largest eigenvalue is $\abs{\frac{1}{\lambda_n}}$
    \end{outline}
\end{definition}

\begin{definition}
    Shifted inverse power iteration iterates on $x$ using $\inv{(A - \sigma I)}$. \\

    The dominant eigenvector corresponds to the eigenvalue $\frac{1}{\lambda' - \sigma}$ where $\lambda'$ is the closest eigenvalue of $A$ to $\sigma$. If $\lambda''$ is the second closest eigenvalue to $A$ then the error rate is given by 

    \[
        \frac{\abs{\lambda' - \sigma}}{\abs{\lambda'' - \sigma}}
    \]

    The cost is initially $O(n^3)$ but then $O(n^2)$ thereafter.

\end{definition}

\begin{definition}
    Shifted Rayleigh iteration leverages both power iteration and the Rayleight quotient method in order to determine the maximal eigenvector:
    \begin{outline}
        \1 Compute the rayleigh quotient to obtain $\sigma_k$.
        \2 Multiply $\inv{(A -\sigma_k I)}$ by $x_k$.
        \3 In reality, by multiplication, we mean compute the
        LU factorizaton of $A - \sigma_k I$ and then use it
        \1 This will presumably give us an eigenvector that corresponds
        to the eigenvector closest to $\sigma$.
        \2 Note that $\sigma$ was not chosen with any prior intent however -- it just happens to be our estimate of the eigenvalue that corresponds to some initial $x$, and that initial $x$ is our estimate of some eigenvector.
        to compute the multiplication.
        \1 This is good, because it can be shown to converge
        at a quadratic rate -- whereas ordinary power iteration
        convergence is linear.
        \2 This is bad, because we have to factor
        $A - \sigma_k I$ every iteration, which can be expensive.
        \2 This may not also converge to an eigenvector we want to converge to apriori.
    \end{outline}
\end{definition}


\begin{definition}
    Simultaneous iteration seeks to determine the maximal $p$ eigenvectors; or rather a basis for them.
    \begin{algo}
        Initialize some $X_0$, an $n \times p$ matrix. \\
        $X_{k+1} = AX_{k}$ \\
        \redcomment{You can perform orthonormalization if needbe} \\
    \end{algo}
\end{definition}

\begin{remark}
    This algorithm is problematic, because the column vectors in $X_k$ may tend to the dominant eigenvector. As a result, $X_k$ becomes increasingly more and more ill conditioned. This is bad, in and of itself, because we typically desire that $X_k \to X$, which we define to be a basis for the span of the dominant $p$ eigenvectors.
\end{remark}

\begin{definition}
    One remedy to this problem is to orthonormalize the iterate of the basis. The resulting algorithm is called orthonormal iteration.

    In what follows, assume that we use the full unreduced $QR$ factorization.

    \begin{algo}
        Initialize some $Q_0$, corresponding to our guess \\
        of a basis for the dominant $p$ eigenvectors.\\
        $Q_0$ is an orthogonal matrix that is $n \times n-p$ \\
        While $\norm{Q_{k+1} - Q_{k}} > \text{some tolerance}$ \+\\
        $X_{k+1} \coloneq AQ_{k}$ \\
        $Q_{k+1}R_{k+1} \coloneq X_{k+1}$ \\
    \end{algo}

    Notice that at each iteration, we have $A = Q_{k+1}R_{k+1}Q_{k}^T$. If $Q_k \to Q$ then $A = QRQ^{T}$ ($R$ also converges as a consequence), so that we have found some (allegedy dominant) $p$ eigenvectors in $Q$ and their eigenvalues in the $p \times p$ upper triangular $R$.
\end{definition}


\begin{remark}
    Assume now that we use the reduced $QR$ factorization so that $Q$ is $n \times r$ and $R$ is $r \times r$.

    The span of $Q_k$ is the span of $X_k$ at any point. Thus $X_{k+1} = A Q_{k}$ gives us a set of column vectors whose span is the same as the span of $AX_{k}$. Thus, we can use $Q_k$ as a proxy to $X_k$.

    Why is span$(Q_k) = $span$(X_k)$ -- since otherwise, the $QR$ factorization of $X_k$ (which we assume to be of rank $r$ -- or else -- we are not determining distinct eigenvectors) will reduce the rank of $X_k$.
\end{remark}



\begin{definition}
    In $QR$ iteration, we decompose $A_k$ as $Q_kR_k$ and then obtain $A_{k+1} = R_kQ_k$. Notice that $A_{k+1} = Q_{k}^TA_kQ_k$ so that if $Q_k$ converges then $A_{k+1}$ is now (this is not yet underestood) really an upper triangular matrix with eigenvalues along its diagonal.
\end{definition}

\begin{remark}
    It is instructive to look at the iterates of simultaneous iteration and $QR$ iteration:

    \begin{align*}
        \hat{Q_1}R_1 = X_0 \\
        X_1 = A \hat{Q_1} \\
        \hat{Q_2}R_2 = X_1 \\
        X_2 = A \hat{Q_2} \\
        \hat{Q_3}R_3 = X_2
        \implies \hat{Q_3} R_3 \hat{Q_2}^H = A
        \implies \hat{Q_{k+1}} R_{k+1} \hat{Q_k}^H = A
    \end{align*}

    QR iteration:

    \begin{align*}
        Q_1R_1 = A_0 = A \\
        A_1 = R_1Q_1 \\
        Q_2R_2 = A_1\\
        A_2 = R_2Q_2 \\
        \implies A_2 = Q_2^H A_1 Q_2 \\
        \implies A_{k+1} = Q_{k+1}^H A_k Q_{k+1} \\
    \end{align*}

    There is an equivalence between the two forms of iteration that can be expressed as follows:

    Suppose that we have $\hat{Q_{k-1}}$ and that we wish to form $X_{k-1} = A \hat{Q_{k-1}}$ and then factorize the resultant product as $\hat{Q_k}R_k$. Assume further that have already factorized $A_{k-1}$ as $Q_k R_k$. Then we can compute $X_{k-1}$ without any work that we would normally do for simultaneous iteration:

    \[
        X_{k-1} = A \hat{Q_{k-1}} = \hat{Q_{k-1}}\hat{Q_{k-1}}^H A \hat{Q_{k-1}} = \hat{Q_{k-1}}A_{k-1} = \hat{Q_{k-1}Q_k R_k}
    \]

    If we set $\hat{Q_k} \coloneq \hat{Q_{k-1}Q_k}$, and appeal to the uniquess of $QR$, then we are done.

\end{remark}
\begin{definition}
    In $QR$ iteration with shifting, we factor $A_k - \sigma_k I$ as $Q_k R_k$ and then compute

    \[
        A_{k+1} = R_k Q_k + \sigma I
    \]

    Note that since $R_k = Q_k^T(A_k - \sigma_k I)$, it follows that

    \[
        A_{k+1} = Q_k^T(A_k - \sigma_k I)Q_k + \sigma I = Q_k^T A_k Q_k
    \]
\end{definition}

\begin{remark}
    Once again, it is instructive to see what the iterates of $QR$ iteration with shifting look like:

    \begin{align*}
        Q_1R_1 = (A_0 - \sigma_0 I) \\
        A_1 = R_1Q_1 + \sigma_0 I \\
        \implies A_{k+1} = Q_{k+1}^H(A_{k} - \sigma_{k}I)Q_{k+1} + \sigma_0 I \\
        = Q_{k+1}^H A_{k} Q_{k+1} \\
    \end{align*}
\end{remark}




\begin{definition}
    The Schur decomposition of a matrix $A = QUQ^H$ expresses $A$ as similar to an upper triangular matrix $U$. A proof of this theorem is given in \url{http://people.inf.ethz.ch/arbenz/ewp/Lnotes/chapter2.pdf} at page 6. We now list some properties pertaining to the matrix:

    \begin{outline}
        \1 Since $AQ = QU$, we can conclude that $AQ_1 = Q_1(U_{1,1})$. From this, it follows that the first schur vector $Q_1$ is an eigenvector.
        \1 If $Q$ and $U$ are real, then $QU$ is a $QR$ decomposition -- obviously.
        \1 If $A$ is real symmetric, then it must be true that $U$ is diagonal, in which case it contains all the eigenvalues of $A$. 
        \2 Moreover, since $U = Q^HAQ$, one can shown that $U^{*} = U$, implying that all the eigenvalues are real. A proof near the proof listed above additionally explains that eigenvectors corresponding to distinct eigenvalues are orthonormal.
    \end{outline}
\end{definition}

\begin{proposition}
    With the schur form of a matrix, we can also use the triangular matrix $U$ to construct eigenvectors.
\end{proposition}

\begin{proof}
    Subtract $\lambda I$ from $U$. Then we get

    \[
        \begin{bmatrix}
            U_{1,1} u U_{1,3} \\
            0 v^T \\
            U_{3,1}
        \end{bmatrix}
    \]

    Note that $U_{1,1}$ is also triangular, $u$ is a column vector, $U_{1,3}$ a rectangular block, $v$ a column vector and $U_{3,1}$ another triangular matrix. Then 
    \[
        \begin{bmatrix}
            -\inv{U_{1,1}} u \\
            1 \\
        \end{bmatrix}
    \]

    is an eigenvector.
\end{proof}

\section{Lecture 13}

\begin{problem}
    Suppose we are given the matrix

    \[
        A = \begin{bmatrix}
            1 & 0 \\
            3 & 0.1 \\
        \end{bmatrix}
    \]

    Then how many power iterations do we need in order to reduce the error by $10^{-10}$.
\end{problem}
\begin{solution}
    The convergence rate is given by $\abs{\frac{\lambda_2}{\lambda_1}}$; hence the rate is $10^{-1}$ and we need at least $10$ iterations. Note that we additionally assume that the starting vector has components in both directions corresponding to the eigenvalue.
\end{solution}

\begin{problem}
    Assume that we are given an $LU$ factorization from a previous computation. What is the cost of performing inverse power iteration for $k$ iterations on an $n \times n$ matrix.
\end{problem}

\begin{solution}
    The cost is $O(kn^2)$. Recall that $LU$ factorizations, once calculated, allow us to solve inverse problems in $O(n^2)$ time.
\end{solution}

\begin{problem}
    Estimate the rayleight quotient of a given matrix for a given eigenvector. Trivial; whence the solution is ommitted.
\end{problem}

\begin{problem}
    Properties of the Schur Form
 Consider the Schur decomposition of a matrix A=QUQH where U is upper-triangular and Q is unitary. The Schur vectors are the columns of Q. While of the following are true?

Select all that apply:
If A is real, Q is orthogonal and U is real
The first Schur vector is an eigenvector of A
The Schur form of a matrix is unique so long as its eigenvalues are not all the same
If A is real symmetric, the Schur decomposition is the eigenvalue decomposition of A
If Q and U are real, QU is a QR decomposition of AQ
    
\end{problem}
\begin{solution}
    \begin{outline}
\1 If A is real, Q is orthogonal and U is real
\2 $A$ coudl be real but $Q$ and $U$ still complex.
\1 The first Schur vector is an eigenvector of A
\2 True -- see above.
\1 The Schur form of a matrix is unique so long as its eigenvalues are not all the same
\2 In the proof of the schur decomposition, the eigenvalue (which is entry occupying the first diagonal entry of $\Lambda$)  was chosen without any specificity -- so that any eigenvalue can be chosen there.
\1 If A is real symmetric, the Schur decomposition is the eigenvalue decomposition of A
\2 Yes, see above.
\2 If Q and U are real, QU is a QR decomposition of AQ.
\2 Yes, can be observed from the form of the schur decomposition itself.
    \end{outline}
    
\end{solution}

\begin{problem}
    QR Iteration for Special Matrices
Which of the following statements about doing one iteration of QR decomposition on a matrix A∈ℝn×n is true?

Select all that apply:
For a Hessenberg matrix A, it takes O(n2) time.
For a Hessenberg matrix A, it takes O(n3) time.
For a tridiagonal matrix A, it takes O(n) time.
For a tridiagonal matrix A, it takes O(n2) time.
\end{problem}

\begin{solution}
    Imagine that we use Givens' rotations. Take the $k$th column as an example. After finding the rotation that takes the $k+1$th row to the $k$th row, we need to apply this same rotation to the remaining $n-k$ columns to the right of the $k$th column. Thus, even if the givens' rotation takes $O(n)$ time to both compute and apply its effects to the $k$the column, the cost of its application to the remaining columns results in $O(n^2)$. By contrast, in a tridiagonal matrix, after finding the rotation that ``settles'' the $k$th column, the rotation only need be applied to the $k+1$th column, since the rotation will change the $k$th entry of that column; no other column to the right will be affected. Thus, this results in an $O(n)$ cost.
\end{solution}

\begin{remark}
    To arrive at the schur form of a matrix, we need to perform some unspecified number of $QR$ factorizations. Each factorization takes $O(n^3)$. Thus the cost can often near $O(n^4)$.
\end{remark}

\begin{remark}
    A better technique is to use a trick involving Householder Transformations:

    \begin{outline}
        \1 Having computed a householder vector $h_i$ that nullifies everything beneath and including row $i+2$, and having formed $H_i$ which is the transformation that achieves this, carry out $H_i A (H_i)^T$. What will this do? It can be shown that this will zero out all entries past and including the $i+2$ row. Doing this for each column, we will attain Hessenberg form. Having attained Hessenberg form. That is $A = \prod_{i=1}^{n}(H_i)^T H (\prod_{i=1}^{n}H_i)$,which is a similarity transform to $A$, meaning that the eigenvalues for $A$ are contained in $H$. We can now perform $QR$ iteration on $H$. It costs $O(n^2)$ to perform the necessary number of Givens rotations to $QR$ factorize the matrix. Suppose that we obtain at the very first iteration the factors $Q_1R_1 = H$. Observe that $A_{2} = R_1Q_1 = R_1H\inv{R_1}$. Further reacall that post or pre multiplying a Hessenberg matrix by an upper triangular matrix preserves the form of the Hesenberg matrix. It therefore follows that $A_2$ is Hessenberg as well; thus every $QR$ iteration now requires $O(n^2)$ as opposed to $O(n^3)$.
    \end{outline}
\end{remark}

\begin{remark}
    In a Krylov subspace method, we restrict our focus to finding a few eigenvect   
\end{remark}<++>

\begin{definition}
    \begin{outline}
        \1 Suppose that we begin with an initial vector $x_0$. Let

        \[
            K_n = \begin{bmatrix}
                x_0 & Ax_0 & \dots & A^{n-1}x_0   \\
            \end{bmatrix}
        \]

        Observe  that 

        \begin{align*}
            AK_n  = \begin{bmatrix}
                   Ax_0 & A^2x_0 & \dots & A^{n}x_0   \\
            \end{bmatrix}
            = K_n
            \underbrace{\begin{bmatrix}
                e_2 & e_3 & \dots & e_n &  \inv{K_n}A^nx_0
            \end{bmatrix}}_{H} 
        \end{align*}

        That is we found that $\inv{K_n}AK_n = H$, where $H$ is upper Hesenberg.
    \end{outline}
\end{definition}

\begin{remark}
    So what? Why is this important? We wish to work with $H$ to determine its eigenvalues.
    The foregoing reveals that we will need to compute $K_n$ if we wish to compute $H$. The problem is that $K_n$ will tend to multiples of the dominant eigenvector of $A$, since $K_n$ is obtained by performing power iteration a few times on $x_0$. This makes $K_n$ highly ill conditioned, so that the Hesenberg form that we would obtain by mat-mat multiplication is likely not useful. What we will therefore do is that we will find a $Q_nR_n$ factorization for $K_n$.
\end{remark}

\begin{remark}
    \begin{align*}
        Q_n^HAQ_n = \inv{(K_nR_n)} A (K_nR_n) = \inv{(R_n)}\inv{K_n}K_nK_n R_n = \inv{(R_n)}HR_n \cong C
    \end{align*}
    17

    where $C$ is some Hesenberg matrix in addition to $H$.
\end{remark}

\begin{proposition}
    The vectors of $Q_n$ can each be computed alone.
\end{proposition}
\begin{proof}
    \begin{align*}
        AQ_n = Q_nC \\
        \intertext{For notational convenience, let $Q =Q_n$} \\
        \intertext{Let us focus on the $j$th column. Suppose that $Q_n$ = }
            \begin{bmatrix}
                q_1 & q_2 \dots q_n
            \end{bmatrix}
            \implies Aq_j = \sum_{i=1}^{j+1}q_{i}C_{i,j} \\
            \intertext{Now solve for $q_2$ assuming that we know $q_1$ and that we have orthonormalized $q_1$. This gives us:}
            \implies Aq_1 = \sum_{i=1}^{2}q_{i}C_{i,1} \\
            \implies Aq_1 = q_{1}C_{1,1} + q_{2}C_{2,1} \\
            \implies Aq_1 - q_{1}C_{1,1} = q_{2}C_{2,1} \\
            \intertext{Note that $Aq_1$ is the ``next'' vector in a Krylov subspace that one would compute with knowledge of $q_0$.} \\
    \end{align*}

    Now in general $C_{i,j} = q_i^HAq_j$. In particular $C_{1,1} = q_1^HAq_1$. If we let $u_1 = (Aq_1)$, then we see that $C_{1,1} = q_1^H u_1$ and that 

    \[
        Aq_1 - q_{1}C_{1,1} = u_1 - q_{1} C_{1,1}
    \]

    This is tantamount to orthogonalizign $u_1$ by removing all of the projections of $u_1$ onto previous $q_k$ (in this case, only $q_1$). Thus if we set $q_2 = \frac{u_1}{\norm{u_1}}$, then we will have effectivel found the vector $q_2$ that is part of the orthonormal set of Krylov space vectors.
\end{proof}

\begin{remark}
    This can be formalized as part of an algorithm:

    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{lecture13_arnoldi}
\end{remark}

\begin{remark}
    Note that this algorithm provides us with a means of obtaining the Hesenberg matrix that the original matrix $A$ is orthogonally similar to. The vectors $q$ are not so important save that we need them in order to determine the values in the matrix $C$ (which stands for the Hesenberg matrix).

    This is done as follows: the crux of this algorithm is the equation

    \[
        Aq_k = \sum_{i=1}^{k+1}C_{ik}q_{i}
    \]

    Assuming that we already know $q_j$ for $j \leq k$, it is our goal to determine $C_{ik}$ for all $i \leq k+1$. To do this, we observe that $q_i^HAq_k = C_{ik}$. Thus, we know all values of $C_{ik}$ where $i \leq k$. To determine $C_{k+1,k}$, observe that

    \[
        \underbrace{Aq_k - \sum_{i=1}^{k}C_{ik}q_{i}}_{\gamma} = C_{k+1, k} q_{k+1}
        = \underbrace{(Aq_k) - \sum_{i=1}^{k}q_i^H (Aq_k)q_{i}}_{\gamma} = C_{k+1, k} q_{k+1}
    \]

    Since $q_{k+1}$ is destined to be part of an orthogonal matrix, we know that $C_{k+1,k}$ is $\norm{\gamma}$ and that $q_{k+1}$ is $\gamma/\norm{\gamma}$.

    Observe that the algorithm above unfolds like Gram Schmidt in that we take the next vector $Aq_k$ and then subtract away the projection of this vector onto previous $q_i$ where $i \leq k-1$.
\end{remark}

\begin{proposition}
    We can compute the eigenvectors corresponding to the $k$ greatest eigenvalues in modulus.
\end{proposition}

\begin{proof}
    We found that $Q_n^h A Q_n \cong H$, an upper Hesenberg matrix. Define 
    \[
        Q_{k} = \begin{bmatrix}
            q_1 & \dots q_k
        \end{bmatrix}
    \]

    to be the $n \times k$ matrix that consists of the first $k$ Arnoldi vectors. Define

    \[
        U_{k} = \begin{bmatrix}
            q_{k+1} & \dots q_{n}
        \end{bmatrix}
    \]

    to be the matrix consisting of the remaining $k$ uncomputed vectors. It follows that

    \begin{align*}
        Q_n^H A Q_n = 
        \begin{bmatrix}
            Q_k^h \\
            U_k^h
        \end{bmatrix}
        A
        \begin{bmatrix}
            Q_k & U_k
        \end{bmatrix}
    \end{align*}
\end{proof}

\section{Lecture 14}{Nonlinear equations}
\begin{problem}
    QR Iteration Convergence
For which of the following classes of matrices will QR iteration always converge and produce all the eigenvalues?

Select all that apply:
Symmetric matrix
Diagonal matrix
Orthogonal matrix
Upper Triangular matrix
General matrix with complex eigenvalues
\end{problem}

\begin{solution}
    Stupid trick question. The answer is a diagonal matrix and an upper triangular matrix. For we don't need to invoke the $QR$ algorithm on these matrices; we already know their eigenvalues (just read the diagonal).
    
    $QR$ is never guaranteed to always converge (ie in a finite number of steps), because there is proveably no algorithm that can give us the eigenvalues for matrices with length at least $5$. 
\end{solution}

\begin{problem}
    Krylov Subspace Conditioning
Given a matrix A with condition number κ=100 and an initial vector x0, how many additional Krylov vectors can be calculated while ensuring the relative error of each vector remains less than or equal to 1×10−6 assuming IEEE double precision.
\end{problem}
\begin{solution}
    The answer is $5$. The relative error is bounded by the condition number multiplied by the relative input error. At the $k$th iteration the relative input error is the relative output error of the $k-1$th iteration where the relative input input error of the $0$th iteration is given by machine epsilon, ie $\approx 2 \times 10^{-16}$. Therefore, relative output error at iteration $k$ is $100^k 2 \times 10^{-16}$, meaning that we can let $k$ be at most $5$.
\end{solution}

\begin{problem}
     Suppose A is a general n×n matrix and Q=[q1q2…qk] is an orthogonal matrix with qj∈Kj(A,b), the Krylov subspace associated with A. How many nonzero elements are in the matrix QTAQ?
\end{problem}
\begin{solution}
    See the notes previously that explain that if we gather the fist $k$ Arnoldi vectors into a matrix $Q$ then $Q^T A Q$ is upper Hesenberg and $k \times k$, meaning it has $k^2$ non zero entries. 
\end{solution}

\begin{problem}
    Qk is the matrix obtained by orthogonalizing the columns of

Ck=[bAbA2b…Ak−1b].
Which of the following is equal to QTkb?
\end{problem}

\begin{solution}
    $Q_k$ will contain column vectors such that only the first vector $q_1$ is orthonormal with $b$. In fact that $\inner{q_1, b} = \inner{q_1, \frac{b}{\norm{b}}\norm{b}} = \norm{b}\inner{q_1, \frac{b}{\norm{b}}} = \norm{b} e_1$.
\end{solution}

\begin{problem}
    Coding question:
\end{problem}

\begin{solution}
    Here the solution is to recall that once we factorize $K_n = Q_nR_n$ then the upper Hesenberg matrix $K_n^T A K_n$ can be alternatively be expressed as

    \[
        Q_n^T A Q
    \]

    since this simplifies to

    \[
        R_n \inv{K_n} A K R_n
    \]

    which is still upper Hesenberg.
\end{solution}

\begin{question}
    Why do we favor symmetric problems in terms of condition number?
\end{question}

\begin{remark}
    Arnoldi is a way to get eigenvalues especially if you cannot store the entire matrix. Why? Since if we compute $Q_k^TAQ_k$, we get a matrix consisting of the first $k$ ritz eigenvalues, which approximate the $k$ eigenvalues greatest in modulus.
\end{remark}

\begin{proposition}
    We can compute the SVD of a square matrix:
\end{proposition}
\begin{proof}
    The following is a naive way of computing the SVD:

    \begin{align*}
        \intertext{Suppose that we have already obtained the eigenvalues and eigenvectors of $A^TA$ as part of a matrix $V$. Note that the eigenvectors constitute an eigenbasis, since $A^TA$ is symmetric and thus has an eigenvector basis. It follows that $A^TA$ is diagonalizable. That is we have:} \\
        A^TA = V \Sigma^2 V^T \\
        \intertext{We know that the diagonal matrix can be represented as the square of a matrix, because $A^TA$ is positive semidefinite and, hence, has non-negative eigenvalues.}
        A = U \Sigma V^T \implies U = A V \inv{\Sigma} \\
        \intertext{Indeed, we will find that $U$ is orthogonal, since} \\
        U^TU = \inv{\Sigma}V^TA^TAV\inv{\Sigma} = I
        \intertext{Note that we seem to have assumed that $A^TA$ is full rank.} \\
    \end{align*}

\end{proof}
    \begin{definition}
        A non linear equation $f: \R^n \to \R$ is solved when we find the tuple $x$ such that $f(x) = 0$. If we are given a function $g: \R^n \to \R$ and we want to find the $x$ such that $g(x) = c$, that we can alternatively find the $x$ such that $f(x) = g(x) - c = 0$.
    \end{definition}

    \begin{remark}
        We have three tools in order to assert the existence of a solution.
        \begin{outline}
            \1 The intermediate value theorem tells us that if $f(a) < 0$ and $f(b) > 0$ and $f$ is continuous, then there is a $c$ such that $a < c < b$ such that $f(c) = 0$.
            \1 The inverse function theorem tells us that if $f: \R^n \to \R$ and the Jacobian of $f$ is non-singular at a point $x$, then there is a neighborhood $B$ of $f(x)$ such that for every $y \in B$, there is an $x$ such that $f(x) = y$.
            \1 \begin{definition}
                A function $g: \R^n \to \R^n$ is a contraction if it holds that for some $0 \leq \alpha < 1$, we have $\norm{g(x) - g(y)} \leq \alpha \norm{x - y}$.
            \end{definition}
            \2 The contraction mapping theorem tells us that if $S$ is a closed set and $g$ a contraction such that $g(S) \s S$, then there is a unique fixed point in $S$.
            \3 For if there were two fixed points, $x$ and $y$, then we would not have $\norm{g(x) - g(y)} \leq \alpha \norm{x-y}.$
            \3 We often reduce root finding problems of the form $f(x) = 0$ to fixed point problems $g(x) = f(x) + x$.
        \end{outline}
    \end{remark}

    \begin{proposition}
        The condition number of solving a root problem is the same as the condition number of evaluating the inverse function at $0$.
    \end{proposition}
    \begin{proof}
        \begin{lemma}
            The condition number of evaluating a function $f$ at $x$ is $\abs{f'(x)}$.
        \end{lemma}
        \begin{proof}
            Suppose that we perturb the input $x$ by $h$; then the relative difference in the output is $f(x+h) - f(x) = hf'(x)$ using the Taylor approximation. It follows that 
            \[
                \abs{\frac{f(x+h) - f(x)}{h}} = \abs{f'(x)}
            \]
            
            
        \end{proof}

        Using the preceding lemma, it follows that the function that we wish to find the absolute condition number of is $x \mapsto \inv{f}(x)$.
        Note that this function has derivative given by $\frac{1}{f'(\inv{f}(x))}$. Whence the condition number is given by

        \[
            \frac{1}{f'(\xa)}
        \]

        where $\xa$ is the root such that $f(\xa) = 0$.

        For this reason, we submitted the proposition above that the condition number is the same as the condition number of evaluating the inverse.
    \end{proof}

    \begin{definition}
        A function has a root of multiplicity $m$ if it holds that
        \[
            f^{j}(x) = 0
        \]

        where $0 \leq j \leq m-1$.
    \end{definition}

    \begin{remark}
        If a function has a root of high multiplicity then evaluating this root has a high condition number. For the function $f$ will be very flat near the root $x$ (since its derivatives at $x$ are all $0$), meaning that $\inv{f}$ will be steep near $0$, which implies that small input perturbations will have a high output perturbation.
    \end{remark}
    \begin{definition}
        Suppose that a function $f$ outputs an estimate $\mu_k$ of some quantity $\mu$ at every iteration $k$. We say that $f$ converges at rate $r$ if

        \[
            \lim_{k \to \infty}\norm{e_{k+1}}/\norm{e_{k}}^r \leq C 
        \]

        where $e_{k} = \mu_k - \mu$ and $0 \leq C < \infty$.
    \end{definition}
    \begin{remark}
        Suppose that $e_0 < 1$ is a starting error for two processes, one quadratically converging and the other linearly converging. In this case, quadratic convergence will converge faster (in fewer iterations) than linear convergence. For linear iteration will decrease the magnitude of the error vector by a constant factor every, say, $k$ iterations whereas quadratic convergence will decrease the magnitude of the error vector by a factor of $2$.
        Quadratic convergence will converge, to finite precision, in no more than $5$ iterations, assuming that the starting error vector is less in magnitude than $10^{0}$, because as we discussed just now, floating point precision will only allow for accuracy for numbers up to $10^{-16}$.

        In light of this quick convergence near the solution, it is not meaningful to have operations with convergence rate greater than $2$.
    \end{remark}

    \begin{definition}
        One of the following criteria for convergence if often employed, although all criterion have their flaws:
        \begin{outline}
            \1 \[
                \abs{f(x)} < \e
            \]
            \2 A very flat function (one with, for example, a root of high multiplicity) may become small at points that are not roots.
            \1 \[
                \norm{x_{k} - x_{k+1}} < \e
            \]
            \2 An algorithm may fail to make progress, accounting for incremental differences.
            \1 \[
                \frac{\norm{x_{k} - x_{k+1}}}{\norm{x_k}} < \e
            \]
            \2 The same as above.
        \end{outline}
    \end{definition}

    \section{Lecture 15}{Bisection Method}

    \pp{1}
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{lecture15_q1}
    Which of the following hold?
    \begin{solution}
        The estimates of the biggest and smallest can only get better and better as we increase $k$. So we find that $M_k$ is a non-decreasing sequence and $m_k$ a non-increasing sequence.
    \end{solution}

    \pp{2}
    Equal Eigenvalues
Assume you are given a matrix whose eigenvalues are all the same. You wish to use Arnoldi iteration to compute the spectrum. What can you say about using this method?
Choice*
Having equal eigenvalues has no effect in the convergence of Arnoldi iteration
Having equal eigenvalues guarrantees the matrix is diagonalizable and therefore Arnoldi always converges
The k-th Ritz value converges to the k-th eigenvalue at iteration k
Arnoldi reduces the matrix to tridiagonal form since having equal eigenvalues means the matrix is symmetric
\begin{solution}
    Having equal eigenvalues gives no informatin about the matrix.
\end{solution}

\pp{3}
Lanczos Iteration
Lanczos iteration modifies Arnoldi iteration when the matrix is symmetric or Hermitian so that recurrence then has only three terms.

Which of the following are properties of Lanczos?

Select all that apply:
Lanczos requires less work than Arnoldi
Lanczos iteration generates an upper Hessenberg reduced Krylov matrix
Lanczos produces a different result than if Arnoldi was applied to the same symmetric matrix
Lanczos iteration generates a tridiagonal reduced Krylov matrix
\begin{solution}
    Lanczos require less work -- since the resulting hesenberg matrix that we fill in the arnoldi vector computation is really tridiagonal then. A tridiagonal matrix is also upper hesenberg, so this works there; lanczos produces no different result -- it just takes less time sinc eit exploits the tridiagonal form; yes the last is a definition.

    Note that the krylov matrix is just the matrix that $A$ is similar to via orthogonalization by an orthogoanl basis of the krylov subspace $[x_0 Ax_0 \dots A^{k-1}x_0]$.
\end{solution}

\pp{4}
2-Norm of a Matrix
Suppose you are given an arbitrary matrix, A, of which you want to compute the 2-norm 
What method would you use to accomplish this in the fastest and most accurate way?

Choice*
Randomly select a set of vectors, x, evenly distributed and apply the definition
Compute the SVD of A directly and read of the largest singular value
Use power iteration to compute the largest eigenvalue which corresponds to the largest singular value
Apply a few iterations of Lanczos to obtain singular value estimate from Krylov subspace of A⊤A
\begin{solution}
    The two norm of a matrix is just the largest singular value; computing the svd takes $O(n^3)$ time -- hitting several random unit vectors by $A$ may not be accurate; the largest eigenvalue does not correspond to the largest singular value unless the matrix is symmetric (note that this is a sufficient condition but not a necessary one -- ie it could be the case that a matrix exists with largest eigenvalue corresponding to the largest singular value, but yet the matrix is not symmetric). Arnoldi iteration takes $O(k^3 + nk^2) = O(\max{k^3, nk^2})$ in general -- since we need $O(nk)$ to orthogonalize the $k$th arnoldi vector against the previous $k-1$ vectors and we need to do this $nk + n(k-1) + nk \approx nk^2$ times. Then when we apply eigenvalue methods to the resulting reduced Arnoldi matrix (the ritz matrix), the costs there is $O(k^3)$. 
\end{solution}

\pp{5}

Just know that the condition number of solving the root problem is the same as teh condition n umber of evaluating the inverse, which is $\frac{1}{f'(\xa)}$ where $\xa$ is a root.

    \begin{definition}
        The bisection method operates on an interval $[a,b]$ where we assume that $sign\left( f(a) \right) =  -sign\left( f(b) \right)$, from which it follows that there exists a zero in $[a,b]$. We then do the following: Letting $m = a + b /2$, if $f(m)f(a) \geq 0$, we recurse on $[m,b]$ or else on $[a,m]$.
    \end{definition}
    \begin{remark}
        At any iteration the distance from either $a$ or $b$ to the root is at most $b - a$. We define $b-a$ to be the error at this iteration. A subsequent iteration will halve the interval, from which we conclude that the error rate is given by
        \[
            e_k \leq e_{k+1}\frac{1}{2}
        \]
    \end{remark}
    \begin{remark}
        Since the magnitude in the error drops by $2^{-1}$ on every iteration, bisection will terminate in at most $52$ iterations, since the error after $52$ iterations is $2^{-52} = \e_{mach}$, and we cannot represent any number thereafter. UNRESOLVED -- well, shouldn't the relative error have to be $2^{-52}$ for us to make this conclusion.
    \end{remark}

    \begin{definition}
        Assume that there is a fixed point $x^{*}$ of a smooth function $g$ (continuously differentiable). Assume further that $g'(x^{*}) < 1$, which implies that $g' < 1$ on a neighborhood of $x^{*}$. Then it follows that

        \[
            g(x_{k}) - g(x^{*}) = g'(\theta)(x_{k} - x^{*})
        \]

    where $g(x_{k}) = x_{k+1}$ and $\theta$ is some point in between $[x_k, x^{*}]$. If $\theta$ falls into the neigborhood where $g' < 1$, then the equation above is a contraction (here, we use the stronger definition) and it converges linearly.
    \end{definition}

    \begin{remark}
        Suppose that $g'(x^{*}) = 0$. Then using a second order taylor expansion, it holds that $g'(\theta) = g'(x^{*}) + g''(x^{*})(\theta - x^{*}) \leq g''(x^{*})(e_k)$. Here we assume that $\theta > x^{*}$, meaning that the fixed point is the point closer to $\infty$. Then it follows that
        \[
            \underbrace{ g(x_{k}) - g(x^{*}) }_{e_{k+1}}\leq g''(\theta)e_k^2
        \]
    \end{remark}


    \begin{remark}
        The fact that $g'(\delta) = g'(x^{\star}) + g''(x^{\star})(x^{\star} - \delta)$ implies that as $\delta$ approaches $x^{*}$, the magnitude of the first derivative drops. In summary if this derivative drops enough that it is $0$ at $x^{*}$, then convergence is quadratic and if, at least, the derivative is less than $1$, then convergence is linear.
    \end{remark}












\end{document}
